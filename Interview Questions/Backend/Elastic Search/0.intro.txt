## Data in: documents and indices

Elasticsearch is a distributed document store. 
It stores complex data structures that have been serialized as JSON documents.

When a document is stored, it is indexed and fully searchable in near real-time (​within 1 second).
Elasticsearch uses a data structure called an inverted index that supports very fast full-text searches.
An inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in.

An index can be thought of as an optimized collection of documents.
Each document is a collection of fields.
Fields are the key-value pairs that contain your data.

Example:
PUT /customer/_doc/1    //customer is the index
{                       //this JSON object is the document
  "name": "John Doe"    //this is a field (key-value pair)
}

By default, Elasticsearch indexes all data in every field and
    each indexed field has a dedicated, optimized data structure.
    e.g. text fields are stored in inverted indices, and
    numeric and geo fields are stored in BKD trees.
The ability to use the per-field data structures
    to assemble and return search results
    is what makes Elasticsearch so fast.

Elasticsearch also has the ability to be schema-less,
    which means that documents can be indexed
    without explicitly specifying how to handle
    each of the different fields that might occur in a document.
When dynamic mapping is enabled,
    Elasticsearch automatically detects and adds new fields to the index.
This default behavior makes it easy to index and explore your data.
    Just start indexing documents and Elasticsearch will detect and map
    booleans, floating point and integer values, dates, and strings
    to the appropriate Elasticsearch datatypes.
But, you can define rules to control dynamic mapping
    and explicitly define mappings to take full control
    of how fields are stored and indexed.

It’s often useful to index the same field in different ways for different purposes.

The analysis chain that is applied to a full-text field
    during indexing is also used at search time.
When you query a full-text field,
    the query text undergoes the same analysis
    before the terms are looked up in the index.

## Information out: search and analyze

The Elasticsearch REST APIs support structured queries, full text queries, and complex queries that combine the two.
Structured queries are similar to the types of queries you can construct in SQL.
Full-text queries find all documents that match the query string and return them sorted by relevance.

Relevance: how good a match a document is for your search terms.

You can access all of these search capabilities
    using Elasticsearch’s comprehensive JSON-style query language (Query DSL).
You can also construct SQL-style queries
    to search and aggregate data natively inside Elasticsearch,
    and JDBC and ODBC drivers enable a broad range of third-party applications
    to interact with Elasticsearch via SQL.

Elasticsearch aggregations enable you
    to build complex summaries of your data and gain insight
    into key metrics, patterns, and trends.

Because aggregations leverage the same data-structures used for search, they are also very fast.
This enables you to analyze and visualize your data in real time.
Your reports and dashboards update as your data changes
    so you can take action based on the latest information.

Aggregations operate alongside search requests.
You can search documents, filter results, and perform analytics
at the same time, on the same data, in a single request.

You can use machine learning features
    to create accurate baselines of normal behavior in your data
    and identify anomalous patterns.
You can do this without having to specify
    algorithms, models, or other data science-related configurations.

## Scalability and resilience

Elasticsearch is built to be always available and to scale with your needs.
It does this by being distributed by nature.
You can add servers (nodes) to a cluster to increase capacity
    and Elasticsearch automatically distributes
    your data and query load across all of the available nodes.
No need to overhaul your application,
    Elasticsearch knows how to balance multi-node clusters
    to provide scale and high availability.
    The more nodes, the merrier.

How does this work?
Under the covers, an Elasticsearch index
    is really just a logical grouping of one or more physical shards, 
    where each shard is actually a self-contained index.
By distributing the documents in an index across multiple shards,
    and distributing those shards across multiple nodes,
    Elasticsearch can ensure redundancy,
    which both protects against hardware failures
    and increases query capacity as nodes are added to a cluster.
As the cluster grows (or shrinks),
    Elasticsearch automatically migrates shards to rebalance the cluster.

There are two types of shards: primaries and replicas.
Each document in an index belongs to one primary shard.
A replica shard is a copy of a primary shard.
Replicas provide redundant copies of your data
    to protect against hardware failure
    and increase capacity to serve read requests
    like searching or retrieving a document.

The number of primary shards in an index
    is fixed at the time that an index is created,
    but the number of replica shards can be changed at any time,
    without interrupting indexing or query operations.

There are a number of performance considerations and trade offs
    with respect to shard size and the number of primary shards configured for an index.
The more shards,
     the more overhead there is simply in maintaining those indices.
The larger the shard size,
    the longer it takes to move shards around when Elasticsearch needs to rebalance a cluster.

Querying lots of small shards makes the processing per shard faster, 
    but more queries means more overhead, so querying a smaller number of larger shards might be faster.
In short…​it depends.
The best way to determine the optimal configuration for your use case
    is through testing with your own data and queries.
But, as a starting point:
    Aim to keep the average shard size
        between a few GB and a few tens of GB.
        For use cases with time-based data,
        it is common to see shards in the 20GB to 40GB range.
    Avoid the gazillion shards problem.
        The number of shards a node can hold is proportional to the available heap space.
        As a general rule, the number of shards per GB of heap space should be less than 20.

For performance reasons, the nodes within a cluster need to be on the same network.
Balancing shards in a cluster across nodes in different data centers simply takes too long.
But high-availability architectures demand that you avoid putting all of your eggs in one basket.
In the event of a major outage in one location, servers in another location need to be able to take over. Seamlessly.
The answer? Cross-cluster replication (CCR).
CCR provides a way to automatically synchronize indices
    from your primary cluster to a secondary remote cluster
    that can serve as a hot backup.
If the primary cluster fails, the secondary cluster can take over.
You can also use CCR to create secondary clusters
    to serve read requests in geo-proximity to your users.
Cross-cluster replication is active-passive.
    The index on the primary cluster is the active leader index and handles all write requests.
    Indices replicated to secondary clusters are read-only followers.